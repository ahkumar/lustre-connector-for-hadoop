# Installation of HAL on Apache Hadoop
This installation guide assumes that user is knowledgable about Lustre* installation and setup.This installation guide assumes that Apache Hadoop 2.3.0 setup is using root user (non-secure).## Step 1: Installing Hadoop Lustre* PluginAssume that `HADOOP_HOME=/usr/lib/hadoop-2.3.0`Copy the file `hadoop-lustre-plugin-2.3.0-ieel-2.0.jar `to Hadoop common library (`$HADOOP_HOME/share/hadoop/common/lib`) directory. This operation needs to be done on all hadoop nodes.`hadoop-lustre-plugin-2.3.0-ieel-2.0.jar` can be found in Intel@ EE for Lustre* package under hadoop directory (install/ieel-2.x.x/hadoop).## Step 2: Prepare Lustre* directory for HadoopCreate a directory on Lustre* that will act as the root directory for Hadoop IO. All relative paths will be computed relate to this base directory. For example, if `/mnt/lustre/hadoop` is Hadoop root directory, then the absolute path `/inputs/data` will be translated to `/mnt/lustre/hadoop/inputs/data`. ## Step 3: Prepare Configuration ParametersThe following configuration changes need to be made on all Hadoop nodes. Additional configuration parameters unrelated to HAL should be included to deploy YARN.Property in core-site.xml | Value | Description--------------------------|-------|-------------fs.defaultFS | lustre:/// | Configure Hadoop to use Lustre* as the default file system. fs.lustre.imp | org.apache.hadoop.fs.LustreFileSystem | Configure Hadoop to use Lustre* Filesystem fs.AbstractFileSystem.lustre.impl | org.apache.hadoop.fs.LustreFileSystem$LustreFs |  Configure Hadoop to use Lustre* class fs.root.dir | <  lustre mount point>/< new-directory-name> (ie: `/mnt/lustre/hadoop`) | Hadoop root directory on Lustre* mount point. Sample core-site.xml```<property>  <name>fs.defaultFS</name>  <value>lustre:///</value></property> <property>  <name>fs.lustre.impl</name>  <value>org.apache.hadoop.fs.LustreFileSystem</value></property><property>  <name>fs.AbstractFileSystem.lustre.impl</name>  <value>org.apache.hadoop.fs.LustreFileSystem$LustreFs</value></property> <property>  <name>fs.root.dir</name>  <value>/mnt/lustre/hadoop</value></property>``` Property in mapred-site.xml | Value | Description----------------------------|-------|-------------mapreduce.map.speculative | false | Turn off map tasks speculative execution (this is incompatible with Lustre* currently) mapreduce.reduce.speculative | false | Turn off reduce tasks speculative execution (this is incompatible with Lustre* currently) mapreduce.job.map.output.collector.class | org.apache.hadoop.mapred.SharedFsPlugins$MapOutputBuffer | Defines the MapOutputCollector implementation to use, specifically for Lustre*, for shuffle phase mapreduce.job.reduce.shuffle.consumer.plugin.class | org.apache.hadoop.mapred.SharedFsPlugins$Shuffle | Name of the class whose instance will be used to send shuffle requests by reducetasks of this job Sample mapred-site.xml```<property>  <name>mapreduce.map.speculative</name>  <value>false</value></property> <property>  <name>mapreduce.reduce.speculative</name>  <value>false</value></property><property>  <name>mapreduce.job.map.output.collector.class</name>  <value>org.apache.hadoop.mapred.SharedFsPlugins$MapOutputBuffer</value></property> <property>  <name>mapreduce.job.reduce.shuffle.consumer.plugin.class</name>  <value>org.apache.hadoop.mapred.SharedFsPlugins$Shuffle</value></property>``` ## Step 4: Verify the Hadoop adapter configurationVerify if the Lustre* Filesystem is working correctly with these commands as root:
```$ hadoop fs -mkdir -p /test1/test2```
Assuming that `fs.root.dir` is set to `/mnt/lustre/hadoop`, the directory `test1` should have been created under `/mnt/lustre/hadoop` and directory test2 should have been created under `test1`.```$ hadoop fs -ls /test1```
It should return directory information `/test1/test2`.## Step 5: Starting YARN and MapReduce JobHistory Serverâ™¦ Note: Make sure you always start ResourceManager before NodeManager. On the ResourceManager node, as root user:```$ $HADOOP_HOME/sbin/yarn-daemon.sh start resourcemanager```
On each NodeManager node, as root user:```$ $HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager```
On the MapReduce JobHistory Server node, as root user:```$ $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver```## Step 6: Run a MapReduce jobRun a test MapReduce job using one of the examples included within the Hadoop distribution, e.g.```$ yarn jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar  pi 4 1000```
The output is like:

```Number of Maps = 4Samples per Map = 1000 Wrote input for Map #0Wrote input for Map #1Wrote input for Map #2Wrote input for Map #3Starting Job......Job Finished in 41.615 secondsEstimated value of Pi is 3.14000000000000000000```
Good luck!